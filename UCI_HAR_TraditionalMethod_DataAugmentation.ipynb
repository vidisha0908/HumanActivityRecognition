{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP. 01. Importing the required libraries to implement traditional data augmentation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import glob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP. 02. IMPLEMENTING DATA AUGMENTATION TECHNIQUES.\n",
    "\n",
    "I HAVE CONSIDERED SEVEN TECHNIQUES:\n",
    "1. JITTER\n",
    "2. RESAMPLE\n",
    "3. TIME WARPING\n",
    "4. MAGNITUDE WARPING\n",
    "5. CROPPING\n",
    "6. PERMUTATION\n",
    "7. ROTATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. JITTER DATA AUGMENTATION TECHNIQUES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCI_Jitter_Tech(UCI_Data_Jitter, sigma=0.01):\n",
    "    UCI_Integer_Values = UCI_Data_Jitter.select_dtypes(include=[np.number])\n",
    "    noise = np.random.normal(loc=0, scale=sigma, size=UCI_Integer_Values.shape)\n",
    "    UCI_Jittered_Data = UCI_Integer_Values + noise\n",
    "    return UCI_Jittered_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCI_Resample_Data(UCI_Data_Resample, Resample_New_Length):\n",
    "    UCI_Integer_Values = UCI_Data_Resample.select_dtypes(include=[np.number])\n",
    "    UCI_Resample_Data = UCI_Integer_Values.apply(lambda x: np.interp(np.linspace(0, len(x) - 1, Resample_New_Length), np.arange(len(x)), x))\n",
    "    return pd.DataFrame(UCI_Resample_Data, columns=UCI_Integer_Values.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCI_Time_Warp_Data(UCI_Data_Time_Warp, sigma=0.2):\n",
    "    \n",
    "    # Considering only integers\n",
    "    UCI_Interger_Values = UCI_Data_Time_Warp.select_dtypes(include=[np.number])\n",
    "    warp = np.cumsum(np.random.normal(loc=1.0, scale=sigma, size=len(UCI_Interger_Values)))\n",
    "    warp = warp / warp[-1]\n",
    "    Time_Warped_Data = pd.DataFrame()\n",
    "    for col in UCI_Interger_Values.columns:\n",
    "        Time_Warped_Data[col] = np.interp(np.linspace(0, 1, len(UCI_Interger_Values)), warp, UCI_Interger_Values[col])\n",
    "    return Time_Warped_Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCI_Magnitude_Warp_Data(UCI_Data_Magnitude_Warp, sigma=0.2):\n",
    "    # only integers\n",
    "    UCI_Interger_Value = UCI_Data_Magnitude_Warp.select_dtypes(include=[np.number])\n",
    "    UCI_Magnitude_Warp = np.random.normal(loc=1.0, scale=sigma, size=UCI_Interger_Value.shape)\n",
    "    UCI_Magnitude_Warped_Data = UCI_Interger_Value * UCI_Magnitude_Warp\n",
    "    return UCI_Magnitude_Warped_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCI_Crop_Data(UCI_Data_Crop, crop_fraction=0.1):\n",
    "    UCI_Interger_Value = UCI_Data_Crop.select_dtypes(include=[np.number])\n",
    "    UCI_Num_To_Crop = int(len(UCI_Interger_Value) * crop_fraction)\n",
    "    start = np.random.randint(0, len(UCI_Interger_Value) - UCI_Num_To_Crop)\n",
    "    UCI_Cropped_Data = UCI_Interger_Value.iloc[start:start+UCI_Num_To_Crop].reset_index(drop=True)\n",
    "    return UCI_Cropped_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCI_Permutation_Data(UCI_Data_Permute,Permute_Num_Segments=4):\n",
    "    # Select only numeric values and calculate segment length\n",
    "    UCI_Integer_Value = UCI_Data_Permute.select_dtypes(include=[np.number])\n",
    "    UCI_Permute_Length = len(UCI_Integer_Value) // Permute_Num_Segments\n",
    "    Permuted_Remainder = len(UCI_Integer_Value) % Permute_Num_Segments\n",
    "    Permuted_Segments = []\n",
    "    # Splitting the data into segments\n",
    "    for i in range(Permute_Num_Segments):\n",
    "        start_idx = i * UCI_Permute_Length\n",
    "        end_idx = (i + 1) * UCI_Permute_Length\n",
    "        if i == Permute_Num_Segments - 1:  # Add the remainder to the last segment\n",
    "            end_idx += Permuted_Remainder\n",
    "        Permuted_Segments.append(UCI_Integer_Value.iloc[start_idx:end_idx])\n",
    "    np.random.shuffle(Permuted_Segments)\n",
    "    UCI_Permuted_Data = pd.concat(Permuted_Segments).reset_index(drop=True)\n",
    "    \n",
    "    return UCI_Permuted_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCI_Rotate_Data(UCI_Data_Rotated, angle=15):\n",
    "    UCI_Interger_Value = UCI_Data_Rotated.select_dtypes(include=[np.number])\n",
    "    rotation_matrix = np.array([[np.cos(np.radians(angle)), -np.sin(np.radians(angle))],\n",
    "                                [np.sin(np.radians(angle)), np.cos(np.radians(angle))]])\n",
    "    Rotation_Augmented_Data = np.dot(UCI_Interger_Value.iloc[:, :2], rotation_matrix)\n",
    "    Rotate_Result = UCI_Interger_Value.copy()\n",
    "    Rotate_Result.iloc[:, :2] = Rotation_Augmented_Data\n",
    "    return Rotate_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCI_Cleaned_Dataset = pd.read_csv('./UCI HAR Dataset/Data/BaseFiles/UCI_Cleaned_Dataset.csv')\n",
    "def saveDataToCSV(ModelsName):\n",
    "    UCI_Cleaned_Dataset_Augmented = UCI_Cleaned_Dataset.copy()\n",
    "    match ModelsName:\n",
    "        case 'JITTER':\n",
    "            UCI_Cleaned_Dataset_Augmented.update(UCI_Jitter_Tech(UCI_Cleaned_Dataset, sigma=0.01))\n",
    "        case 'RESAMPLING':\n",
    "            UCI_Cleaned_Dataset_Augmented.update(UCI_Resample_Data(UCI_Cleaned_Dataset, Resample_New_Length=100))\n",
    "        case 'TIME WRAPPING':\n",
    "            UCI_Cleaned_Dataset_Augmented.update(UCI_Time_Warp_Data(UCI_Cleaned_Dataset_Augmented))\n",
    "        case 'MAGNITUDE WRAPPING':\n",
    "            UCI_Cleaned_Dataset_Augmented.update(UCI_Magnitude_Warp_Data(UCI_Cleaned_Dataset))\n",
    "        case 'CROP DATA':\n",
    "            UCI_Cleaned_Dataset_Augmented.update(UCI_Crop_Data(UCI_Cleaned_Dataset))\n",
    "        case 'PERMUTATION':\n",
    "            UCI_Cleaned_Dataset_Augmented.update(UCI_Permutation_Data(UCI_Cleaned_Dataset) )\n",
    "        case 'ROTATION':\n",
    "            UCI_Cleaned_Dataset_Augmented.update(UCI_Rotate_Data(UCI_Cleaned_Dataset))\n",
    "    UCI_Cleaned_Dataset_Augmented.to_csv('./UCI HAR Dataset/Data/Traditional/UCI_Cleaned_Dataset_Augmented_'+ModelsName+'.csv', index=False, float_format='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDataToCSV('JITTER')\n",
    "saveDataToCSV('RESAMPLING')\n",
    "saveDataToCSV('TIME WRAPPING')\n",
    "saveDataToCSV('MAGNITUDE WRAPPING')\n",
    "saveDataToCSV('CROP DATA')\n",
    "saveDataToCSV('PERMUTATION')\n",
    "saveDataToCSV('ROTATION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the traditional data augmentation technquies to perform the further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = './UCI HAR Dataset/Data/Traditional/UCI_Cleaned_Dataset_Augmented_*.csv'\n",
    "files = glob.glob(pattern)\n",
    "Combined_Traditional_Augmentated = pd.concat((pd.read_csv(f) for f in files), ignore_index=True)\n",
    "Combined_Traditional_Augmentated.to_csv('./UCI HAR Dataset/Data/Traditional/UCI_Cleaned_Dataset_Traditional_Augmented.csv', index=False, float_format='%.2f')\n",
    "print(Combined_Traditional_Augmentated.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP. 04. SETTING SOME CHECKPOINTS TO PREPARE COMBINED TRADITION AUGMENTATED DATASET FOR MODEL TRAINING.\n",
    "1. CHECKING FOR DUPLICATES AND NULL VALUES.\n",
    "2. SPLITTING THE DATASET INTO TRAIN AND TEST.\n",
    "3. CHECKING AND BALANCING THE DATA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP. 04. 1 Checking for duplicates and null value and fixing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check duplicates and null valaues available in new combined datasheet.\n",
    "DuplicateValues = Combined_Traditional_Augmentated.duplicated().sum()\n",
    "print(f\"Number of duplicates in the augmented dataset: {DuplicateValues}\")\n",
    "NullValues = Combined_Traditional_Augmentated.isnull().sum().sum()\n",
    "print(f\"Number of null values in the augmented dataset: {NullValues}\")\n",
    "if DuplicateValues == 0 and NullValues == 0:\n",
    "    print(\"No duplicates or null values found.We are good to proceed!\")\n",
    "else:\n",
    "    print(\"Duplicates or null values found.\")\n",
    "\n",
    "Combined_Traditional_Augmentated = Combined_Traditional_Augmentated.drop_duplicates()\n",
    "print(f\"Shape after removing duplicates: {Combined_Traditional_Augmentated.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP. 04. 2. Data Spliting into 80% train and 20% test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset on the basis of activities performed by the each participant\n",
    "Traditional_Augmented_X_Value = Combined_Traditional_Augmentated.drop(columns=['Activity'])\n",
    "Traditional_Augmented_Y_Value = Combined_Traditional_Augmentated['Activity']\n",
    "label_encoder_augmented = LabelEncoder()\n",
    "Traditional_Augmented_Y_Value_Encoded = label_encoder_augmented.fit_transform(Traditional_Augmented_Y_Value)\n",
    "\n",
    "# Spliting augmented data into 80% for training and 20% for test\n",
    "X_Augmented_train, X_Augmented_test, Y_Augmented_Train, Y_Augmented_test = train_test_split(\n",
    "    Traditional_Augmented_X_Value, Traditional_Augmented_Y_Value_Encoded, test_size=0.2, random_state=42, stratify=Traditional_Augmented_Y_Value_Encoded)\n",
    "non_numeric_columns = X_Augmented_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "for i in non_numeric_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_Augmented_train[i] = le.fit_transform(X_Augmented_train[i])\n",
    "    X_Augmented_test[i] = le.transform(X_Augmented_test[i])\n",
    "label_encoder = LabelEncoder()\n",
    "Y_Augmented_Train_encoded = label_encoder.fit_transform(Y_Augmented_Train)\n",
    "\n",
    "\n",
    "# Saving all the split test and train dataset for the augmented data\n",
    "X_Augmented_train.to_csv('./UCI HAR Dataset/Data/Traditional/UCI_Traditional_Train_Augmented.csv', index=False, float_format='%.2f')\n",
    "X_Augmented_test.to_csv('./UCI HAR Dataset/Data/Traditional/UCI_Traditional_Test_Augmented.csv', index=False, float_format='%.2f')\n",
    "pd.DataFrame(Y_Augmented_Train, columns=['Activity']).to_csv('./UCI HAR Dataset/Data/Traditional/UCI_Traditional_Train_Labels_Augmented.csv', index=False)\n",
    "pd.DataFrame(Y_Augmented_test, columns=['Activity']).to_csv('./UCI HAR Dataset/Data/Traditional/UCI_Traditional_Test_Labels_Augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP. 04. 3. Data Banalcing (Checking and balancing them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SMOTE and apply it to balance the training set\n",
    "UCI_Trad_Smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced_encoded = UCI_Trad_Smote.fit_resample(X_Augmented_train, Y_Augmented_Train_encoded)\n",
    "y_train_balanced = label_encoder.inverse_transform(y_train_balanced_encoded)\n",
    "\n",
    "# Save the balanced training data\n",
    "X_train_balanced.to_csv('./UCI HAR Dataset/Data/Traditional/UCI_Traditional_Train_Balanced_Agumented.csv', index=False, float_format='%.2f')\n",
    "y_train_balanced_df = pd.DataFrame(y_train_balanced, columns=['Activity'])\n",
    "y_train_balanced_df.to_csv('./UCI HAR Dataset/Data/Traditional/UCI_Traditional_Train_Labels_Balanced.csv', index=False)\n",
    "Y_Augmented_Train_series = pd.Series(Y_Augmented_Train)\n",
    "Imbalance_Distribution_DataFrame = pd.DataFrame(Y_Augmented_Train_series.value_counts()).reset_index()\n",
    "Imbalance_Distribution_DataFrame.columns = ['ACTIVITY', 'HEADCOUNTS']\n",
    "Balanced_Distribution_DataFrame = pd.DataFrame(y_train_balanced_df['Activity'].value_counts()).reset_index()\n",
    "Balanced_Distribution_DataFrame.columns = ['ACTIVITY', 'HEADCOUNTS']\n",
    "\n",
    "Imbalance_Distribution_Display = Imbalance_Distribution_DataFrame.style.set_table_styles(\n",
    "    [{'selector': 'th, td', 'props': [('border', '1px solid black')]}]\n",
    ").format(precision=2).set_caption(\"IMBALANCE DISTRIBUTION\")\n",
    "\n",
    "Balanced_Distribution_Display = Balanced_Distribution_DataFrame.style.set_table_styles(\n",
    "    [{'selector': 'th, td', 'props': [('border', '1px solid black')]}]\n",
    ").format(precision=2).set_caption(\"BALANCED DISTRIBUTION\")\n",
    "display(Imbalance_Distribution_Display)\n",
    "display(Balanced_Distribution_Display)\n",
    "\n",
    "# Plotting the graph to compare balanced and imbalanced data\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "sns.countplot(x=Y_Augmented_Train_series, ax=axs[0])\n",
    "axs[0].set_title('IMBALANCED DISTRIBUTION', fontsize=16)\n",
    "axs[0].set_xlabel('ACTIVITIES', fontsize=14)\n",
    "axs[0].set_ylabel('HEADCOUNTS', fontsize=14)\n",
    "axs[0].tick_params(axis='x', rotation=45, labelsize=12)\n",
    "\n",
    "# Plot for Balanced Distribution\n",
    "sns.countplot(x=y_train_balanced_df['Activity'], ax=axs[1])\n",
    "axs[1].set_title('BALANCED DISTRIBUTION', fontsize=16)\n",
    "axs[1].set_xlabel('ACTIVITIES', fontsize=14)\n",
    "axs[1].set_ylabel('HEADCOUNTS', fontsize=14)\n",
    "axs[1].tick_params(axis='x', rotation=45, labelsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP. 05. TRAINING THE MODELS\n",
    "\n",
    "FOR THIS PROJECT I AM SELECTING FOUR BEST SUITED MODELS FOR DATA AUGMNETATION TECHNIQUES STUDY.\n",
    "\n",
    "01. RANDOM FOREST\n",
    "02. SUPPORT VECTOR MACHINE (SVM)\n",
    "03. CONVOLUTIONAL NEURAL NETWORK (CNN)\n",
    "04. LONG SHORT-TERM MEMORY (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndtestdatasheet(modelsName, UCI_X_Train, UCI_X_Test):\n",
    "    if modelsName == 'LSTM':\n",
    "        UCI_X_Train = np.expand_dims(X_train_balanced, axis=1)\n",
    "        UCI_X_Test  = np.expand_dims(X_Augmented_test, axis=1)\n",
    "    else:\n",
    "        UCI_X_Train = np.expand_dims(X_train_balanced, axis=-1)\n",
    "        UCI_X_Test  = np.expand_dims(X_Augmented_test, axis=-1)\n",
    "\n",
    "    return UCI_X_Train, UCI_X_Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consolidating all results in single csv(accuracy,F1 Score)\n",
    "def getAccuracyAndF1Score(Traditional,ModelName,Accuracy,F1Score):\n",
    "    if not os.path.exists('./UCI HAR Dataset/results'):\n",
    "        os.makedirs('./UCI HAR Dataset/results')\n",
    "\n",
    "    result = {\n",
    "        \"Traditional Class\":Traditional,\n",
    "        \"Model\": ModelName,\n",
    "        \"Accuracy\": Accuracy,\n",
    "        \"F1 Score\": F1Score\n",
    "    }\n",
    "    file_path = './UCI HAR Dataset/results/results.csv'\n",
    "    df_result = pd.DataFrame([result])\n",
    "    if os.path.exists(file_path):\n",
    "        df_result.to_csv(file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df_result.to_csv(file_path, mode='w', header=True, index=False)\n",
    "        print(f\"File {file_path} created and results logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainingModelAndGeneratingReport(TrainingModelName,ModelsName):\n",
    "    ModelAccuracy = 0.0\n",
    "    ModelClassificationDataFrame = pd.DataFrame\n",
    "    UCI_X_Train, UCI_X_Test = trainAndtestdatasheet(ModelsName,X_train_balanced, X_Augmented_test)\n",
    "    if (ModelsName =='LSTM' or ModelsName =='CNN'):  \n",
    "        TrainingModelName.fit(UCI_X_Train, y_train_balanced, epochs=10,batch_size=32,validation_data=(UCI_X_Test, Y_Augmented_test))\n",
    "        UCI_Evaluation = TrainingModelName.evaluate(UCI_X_Test, Y_Augmented_test)\n",
    "        ModelPrediction = np.argmax(TrainingModelName.predict(UCI_X_Test), axis=-1)\n",
    "        ModelClassificationReport = classification_report(Y_Augmented_test, ModelPrediction, output_dict=True)\n",
    "        ModelClassificationDataFrame = pd.DataFrame(ModelClassificationReport).transpose()\n",
    "        ModelClassificationDataFrame.loc['accuracy'] = [UCI_Evaluation[1] * 100, '-', '-', '-']\n",
    "        ModelAccuracy = accuracy_score(Y_Augmented_test, ModelPrediction) * 100\n",
    "        ModelF1Score = f1_score(Y_Augmented_test, ModelPrediction, average='weighted')\n",
    "    else:\n",
    "        TrainingModelName.fit(X_train_balanced, y_train_balanced)\n",
    "        ModelPrediction = TrainingModelName.predict(X_Augmented_test)\n",
    "        ModelAccuracy = accuracy_score(Y_Augmented_test, ModelPrediction) * 100\n",
    "        ModelF1Score = f1_score(Y_Augmented_test, ModelPrediction, average='weighted')\n",
    "        ModelClassificationReport = classification_report(Y_Augmented_test, ModelPrediction, output_dict=True)\n",
    "        ModelClassificationDataFrame = pd.DataFrame(ModelClassificationReport).transpose()\n",
    "        ModelClassificationDataFrame.loc['accuracy'] = [ModelAccuracy, '-', '-', '-']\n",
    "    getAccuracyAndF1Score('Traditional Class',ModelsName,ModelAccuracy,ModelF1Score)\n",
    "    ModelClassificationDataFrame = ModelClassificationDataFrame.round(2)\n",
    "    ModelClassificationDataFrame.columns = [col.upper() for col in ModelClassificationDataFrame.columns]\n",
    "    ModelClassificationDataFrame.index = [idx.upper() for idx in ModelClassificationDataFrame.index]\n",
    "\n",
    "    #Displaying report\n",
    "    Classification_Report_Display = ModelClassificationDataFrame.style.set_table_styles(\n",
    "    [{'selector': 'th, td', 'props': [('border', '1px solid black')]}]\n",
    "    ).format(precision=2)\n",
    "    display(ModelsName+': Classification Report.')\n",
    "    display(Classification_Report_Display)\n",
    "\n",
    "    #Putting confusion matrix\n",
    "    ModelConfusionmatrix= confusion_matrix(Y_Augmented_test, ModelPrediction)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(ModelConfusionmatrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=np.unique(y_train_balanced), yticklabels=np.unique(y_train_balanced))\n",
    "    plt.title(ModelsName + ' UCI : Confusion Matrix')\n",
    "    plt.xlabel('PREDICTED LABEL')\n",
    "    plt.ylabel('TRUE LABEL')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callModels(ModelsName):\n",
    "    print(\"\\033[1;34m.....................................................!\\033[0m\")\n",
    "    print('Running for Traditional '+ModelsName+'.....................>>>>>')\n",
    "    print(\"\\033[1;34m.....................................................!\\033[0m\")\n",
    "    UCI_X_Train, UCI_X_Test = trainAndtestdatasheet(ModelsName,X_train_balanced, X_Augmented_test)\n",
    "    match ModelsName.upper():\n",
    "        case 'RANDOM FOREST':\n",
    "            TrainingModelName = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            TrainingModelAndGeneratingReport(TrainingModelName,ModelsName)\n",
    "            \n",
    "        case 'SVM':\n",
    "            TrainingModelName = SVC(kernel='linear', random_state=42)\n",
    "            TrainingModelAndGeneratingReport(TrainingModelName,ModelsName)\n",
    "        case 'LSTM':\n",
    "            TrainingModelName = models.Sequential([\n",
    "               layers.LSTM(64, input_shape=(UCI_X_Train.shape[1], UCI_X_Train.shape[2])),\n",
    "               layers.Dense(128, activation='relu'),\n",
    "               layers.Dense(len(np.unique(y_train_balanced)), activation='softmax')\n",
    "            ])\n",
    "\n",
    "            TrainingModelName.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            TrainingModelAndGeneratingReport(TrainingModelName,ModelsName)\n",
    "\n",
    "        case 'CNN': \n",
    "            TrainingModelName = models.Sequential([\n",
    "            layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=(UCI_X_Train.shape[1], 1)),\n",
    "            layers.MaxPooling1D(pool_size=2),\n",
    "            layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "            layers.MaxPooling1D(pool_size=2),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(len(np.unique(y_train_balanced)), activation='softmax')  # Assuming y is numeric with unique values\n",
    "            ])\n",
    "            # Compile with a smaller learning rate and Adam optimizer\n",
    "            TrainingModelName.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            TrainingModelAndGeneratingReport(TrainingModelName,ModelsName)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1;32m.........................TRAINING TRADITIONAL............................!\\033[0m\")\n",
    "callModels('RANDOM FOREST')\n",
    "callModels('SVM')\n",
    "callModels('LSTM')\n",
    "callModels('CNN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
